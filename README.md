# Fool_Network_Papers
A Paper Collection of Fooling the Deep Neural Network

### Done:
1. [Adversarial examples in the physical world](http://cn.arxiv.org/abs/1607.02533) 
**(ICLR2017 Workshop)**

2. [Ian Speech on CS231n](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture16.pdf)

3. [DeepFool: a simple and accurate method to fool deep neural networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.pdf)
**(CVPR2016)**
The idea in this work is close to the orginal idea. 
Loop until the predicted label change.

4. [Learning with a strong adversary](http://cn.arxiv.org/pdf/1511.03034.pdf)
**(rejected by ICLR2016?)** Apply the spirit of GAN to optimization.

5. [Decision-based Adversarial Attacks: Reliable Attacks Against Black-box Machine Learning Models](http://cn.arxiv.org/pdf/1712.04248.pdf)
**ICLR2018** [[code]](https://github.com/bethgelab/foolbox)

6. [The limitations of deep learning in adversarial settings](https://arxiv.org/pdf/1511.07528.pdf) **ESSP** (European Symposium on Security & Privacy) Propose SaliencyMapAttack. Do not use loss function.

7. [Generating Natural Adversarial Examples](https://openreview.net/forum?id=H1BLjgZCb&noteId=r1dkEyaSG) **ICLR2018**

### To Read:
1. [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)
**(ICLR2015)** Fast Gradient Sign Method

2. [Exploring the space of adversarial images](http://ieeexplore.ieee.org/document/7727230/)
**IJCNN**

3. [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083) **ICLR2018**

4. [Stochastic Activation Pruning for Robust Adversarial Defense](https://openreview.net/forum?id=H1uR4GZRZ) **ICLR2018**

5. [Mitigating Adversarial Effects Through Randomization](https://openreview.net/forum?id=Sk9yuql0Z) **ICLR2018**

6. [Ensemble Adversarial Training: Attacks and Defenses](https://openreview.net/forum?id=rkZvSe-RZ) **ICLR2018**
